{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puP3oVKLxrVP",
        "outputId": "c8deb489-ba39-49ce-c0c1-e098764be40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_excel('d.xlsx')\n",
        "d_typo=pd.read_excel('Typo.xlsx')"
      ],
      "metadata": {
        "id": "82OaAxmSx6v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_spell=pd.read_excel('spelling.xlsx')\n",
        "d_ambi=pd.read_excel('ambiguity.xlsx')"
      ],
      "metadata": {
        "id": "7jB9jwM1yP8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(d_ambi, test_size=0.7, random_state=38)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=38)"
      ],
      "metadata": {
        "id": "0Trjx9sryVfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmiBfJkuygnL",
        "outputId": "d94d31ba-8a07-46bd-fee5-e36fafe78256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_df['Sentence']=val_df['Sentence'].astype(str)\n",
        "val_df = val_df.dropna(subset=['Label_id'])\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "N6-7Ri0h05Yz",
        "outputId": "7e582f94-48c1-4480-d051-feb29115fb21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Sentence  Label_id Language\n",
              "0    নীতিগুলি সকল অভিবাসীদের জন্য সামাজিক ও অর্থনৈত...         7  Bengali\n",
              "1    Commission income rose by 25.7 % to EUR 16.1 m...        12  English\n",
              "2    The operating profit for Grain Trading increas...        12  English\n",
              "3    స్వలింగ వివాహాలు అభివృద్ధి చెందుతున్న సాంస్కృత...         1   Telugu\n",
              "4    स्वदेशी तथा विदेशीसँग ऋण लिएर सरकारहरू चल्न था...         5   Nepali\n",
              "..                                                 ...       ...      ...\n",
              "994  Resource managers can reassign tasks modify da...        12  English\n",
              "995  স্বাস্থ্য এবং নিরাপত্তা এবং অভিবাসন সম্পর্কে ক...         7  Bengali\n",
              "996  सरकारले नयाँ राहत दिन नसक्ने अवस्था उत्पन्न भय...         5   Nepali\n",
              "997  Finnish Talentum reports its operating profit ...        12  English\n",
              "998  हुन सक्छ त्यसले त्यस्तो म्यासेज कसैलाई आएमा त्...         5   Nepali\n",
              "\n",
              "[999 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d23423c4-c52b-464f-bb19-2a183882ecc0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label_id</th>\n",
              "      <th>Language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>নীতিগুলি সকল অভিবাসীদের জন্য সামাজিক ও অর্থনৈত...</td>\n",
              "      <td>7</td>\n",
              "      <td>Bengali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Commission income rose by 25.7 % to EUR 16.1 m...</td>\n",
              "      <td>12</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The operating profit for Grain Trading increas...</td>\n",
              "      <td>12</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>స్వలింగ వివాహాలు అభివృద్ధి చెందుతున్న సాంస్కృత...</td>\n",
              "      <td>1</td>\n",
              "      <td>Telugu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>स्वदेशी तथा विदेशीसँग ऋण लिएर सरकारहरू चल्न था...</td>\n",
              "      <td>5</td>\n",
              "      <td>Nepali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>Resource managers can reassign tasks modify da...</td>\n",
              "      <td>12</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>স্বাস্থ্য এবং নিরাপত্তা এবং অভিবাসন সম্পর্কে ক...</td>\n",
              "      <td>7</td>\n",
              "      <td>Bengali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>सरकारले नयाँ राहत दिन नसक्ने अवस्था उत्पन्न भय...</td>\n",
              "      <td>5</td>\n",
              "      <td>Nepali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Finnish Talentum reports its operating profit ...</td>\n",
              "      <td>12</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>हुन सक्छ त्यसले त्यस्तो म्यासेज कसैलाई आएमा त्...</td>\n",
              "      <td>5</td>\n",
              "      <td>Nepali</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>999 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d23423c4-c52b-464f-bb19-2a183882ecc0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d23423c4-c52b-464f-bb19-2a183882ecc0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d23423c4-c52b-464f-bb19-2a183882ecc0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# create an instance of the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_s=train_df['Label_id']\n",
        "val_s=val_df['Label_id']\n",
        "test_s=train_df['Label_id']\n",
        "# fit the encoder to your labels and transform them to numerical labels\n",
        "train_labels = label_encoder.fit_transform(train_s)\n",
        "val_labels = label_encoder.transform(val_s)\n",
        "test_labels = label_encoder.transform(test_s)\n",
        "\n",
        "# check the unique values of your encoded labels\n",
        "print(set(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIqLT96P4ppO",
        "outputId": "9f851ac3-ae24-4f2e-cb3a-61f03a392955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "# Load the mBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Define the maximum sequence length (this should match the value used during training)\n",
        "max_length = 75\n",
        "\n",
        "# Prepare the input data\n",
        "def prepare_input(text, language):\n",
        "    # Concatenate the text and language information\n",
        "    input_text = f\"{text} [{language}]\"\n",
        "\n",
        "    # Encode the concatenated string using the tokenizer\n",
        "    encoded = tokenizer(\n",
        "        input_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "    return encoded\n",
        "\n",
        "\n",
        "# Load your dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Prepare the input data for prediction\n",
        "texts = data['Sentence'].tolist()\n",
        "languages = data['Language'].tolist()\n",
        "\n",
        "encoded = [prepare_input(text, lang) for text, lang in zip(texts, languages)]\n",
        "input_ids = tf.stack([enc['input_ids'] for enc in encoded])\n",
        "attention_mask = tf.stack([enc['attention_mask'] for enc in encoded])\n",
        "languages = tf.constant(languages, shape=(len(languages), 1))\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_name = 'bert-base-multilingual-cased'\n",
        "num_labels = 12\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# Predict the class probabilities for each input\n",
        "outputs = model({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
        "logits = outputs.logits\n",
        "class_probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "# Get the predicted class for each input\n",
        "predicted_classes = tf.argmax(class_probs, axis=-1)\n",
        "\n",
        "# Convert the predicted class tensor to a numpy array\n",
        "predicted_classes = predicted_classes.numpy()\n",
        "\n",
        "# Print the predicted classes\n",
        "#print(predicted_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGRbPow84yS3",
        "outputId": "b3b45fcd-811e-4430-c007-97b458a3462c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k=data['Label_id']\n",
        "# Compute the accuracy of the model predictions\n",
        "num_correct = (predicted_classes == k).sum()\n",
        "accuracy = num_correct / len(train_labels)\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAPxX5F_1Bjw",
        "outputId": "7f4bbcf4-cd0c-4508-8087-07a0950e9cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.49760765550239233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# k: true labels\n",
        "# predicted_classes: predicted labels\n",
        "recall = recall_score(k, predicted_classes, average='macro')\n",
        "print('Recall:', recall)\n"
      ],
      "metadata": {
        "id": "FLuRRq8Z17Uk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d50cf86-479c-41bc-b8d2-d341a62b51ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.08333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "k = data['Label_id']\n",
        "precision = precision_score(k, predicted_classes, average=None)\n",
        "\n",
        "# If you want to compute the micro/macro/weighted average precision across all classes, you can set the 'average' parameter:\n",
        "precision_micro = precision_score(k, predicted_classes, average='micro')\n",
        "precision_macro = precision_score(k, predicted_classes, average='macro')\n",
        "precision_weighted = precision_score(k, predicted_classes, average='weighted')\n",
        "print(precision_micro)\n",
        "print(precision_macro)\n",
        "print(precision_weighted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGCz_qxd4w2z",
        "outputId": "86bd4ded-002e-48ed-e4a4-1b7b78e36161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1041041041041041\n",
            "0.008675342008675343\n",
            "0.010837664491318144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer, TFXLMRobertaForSequenceClassification\n",
        "from transformers import XLMRobertaTokenizerFast\n",
        "\n",
        "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "\n",
        "\n",
        "# Define the maximum sequence length (this should match the value used during training)\n",
        "max_length = 128\n",
        "\n",
        "# Prepare the input data\n",
        "def prepare_input(text):\n",
        "    # Encode the text using the tokenizer\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "    return encoded\n",
        "\n",
        "# Load your dataset and prepare the input data for prediction\n",
        "# ...\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_name = 'xlm-roberta-base'\n",
        "num_labels = 12\n",
        "model = TFXLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# Predict the class probabilities for each input\n",
        "outputs = model({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
        "logits = outputs.logits\n",
        "class_probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "# Get the predicted class for each input\n",
        "predicted_classes = tf.argmax(class_probs, axis=-1)\n",
        "\n",
        "# Convert the predicted class tensor to a numpy array\n",
        "predicted_classes = predicted_classes.numpy()\n",
        "\n",
        "# Print the predicted classes\n",
        "#print(predicted_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydlHRQ5gca6m",
        "outputId": "aa8f487f-2190-464b-9f21-aeaa951e239d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFXLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k=data['Label_id']\n",
        "# Compute the accuracy of the model predictions\n",
        "num_correct = (predicted_classes == k).sum()\n",
        "accuracy = num_correct / len(train_labels)\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "thYM6L0p3YxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd929ef2-c314-4b6a-9f01-771353deb1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.1722488038277512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Robustness check on the model : typo errors\n"
      ],
      "metadata": {
        "id": "z6j-FHHsVD1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Function to add random typos to a word\n",
        "def add_typos(word):\n",
        "    # List of possible typo characters\n",
        "    typo_chars = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    # Choose a random number of typos to add (up to 2)\n",
        "    num_typos = random.randint(0, 2)\n",
        "    # Add typos to the word\n",
        "    for i in range(num_typos):\n",
        "        # Choose a random position in the word to add the typo\n",
        "        pos = random.randint(0, len(word)-1)\n",
        "        # Choose a random typo character\n",
        "        typo_char = random.choice(typo_chars)\n",
        "        # Add the typo character to the word\n",
        "        word = word[:pos] + typo_char + word[pos+1:]\n",
        "    return word\n",
        "\n",
        "# Read the Excel file into a pandas dataframe\n",
        "df = pd.read_excel('d.xlsx')\n",
        "\n",
        "# Iterate over each cell in the dataframe\n",
        "for index, row in df.iterrows():\n",
        "  if isinstance(df['Sentence'], str):\n",
        "            # Add typos to the text in the cell\n",
        "    df.at[index, df['Sentence']] = add_typos(df['Sentence'])\n",
        "\n",
        "# Write the modified dataframe to a new Excel file\n",
        "df.to_excel('Typo.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "DvFRQkSNU5qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Robustness check on the model : Spelling error"
      ],
      "metadata": {
        "id": "IYw3QyQc6BO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyahocorasick\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXlZ3Rpc6xEU",
        "outputId": "34194b48-0ade-4a7f-e693-2b9a9c2436fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick\n",
            "Successfully installed pyahocorasick-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import ahocorasick\n",
        "\n",
        "# Load dictionary of correct words\n",
        "with open('dict.txt') as f:\n",
        "    words = set(f.read().split())\n",
        "\n",
        "# Create Aho-Corasick automaton for fast pattern matching\n",
        "AC = ahocorasick.Automaton()\n",
        "for word in words:\n",
        "    AC.add_word(word, word)\n",
        "AC.make_automaton()\n",
        "\n",
        "# Function to add random spelling error to a word\n",
        "def add_spelling_error(word):\n",
        "    # Find all possible corrections for the word\n",
        "    corrections = set()\n",
        "    for end_index, _ in AC.iter(word):\n",
        "        corrections.add(word[:end_index])\n",
        "    if len(corrections) == 0:\n",
        "        # No corrections found, so return the original word\n",
        "        return word\n",
        "    # Choose a random correction for the word\n",
        "    new_word = random.choice(list(corrections))\n",
        "    # Add a random letter deletion, insertion, or substitution\n",
        "    operation = random.choice(['delete', 'insert', 'substitute'])\n",
        "    pos = random.randint(0, len(new_word)-1)\n",
        "    if operation == 'delete' and len(new_word) > 1:\n",
        "        new_word = new_word[:pos] + new_word[pos+1:]\n",
        "    elif operation == 'insert':\n",
        "        new_word = new_word[:pos] + random.choice('abcdefghijklmnopqrstuvwxyz') + new_word[pos:]\n",
        "    elif operation == 'substitute':\n",
        "        new_word = new_word[:pos] + random.choice('abcdefghijklmnopqrstuvwxyz') + new_word[pos+1:]\n",
        "    return new_word\n",
        "\n",
        "# Read the Excel file into a pandas dataframe\n",
        "df = pd.read_excel('d.xlsx')\n",
        "\n",
        "# Iterate over each cell in the dataframe\n",
        "for index, row in df.iterrows():\n",
        "  if isinstance(df['Sentence'], str):\n",
        "            # Add typos to the text in the cell\n",
        "    df.at[index, df['Sentence']] = add_typos(df['Sentence'])\n",
        "\n",
        "# Write the modified dataframe to a new Excel file\n",
        "df.to_excel('spelling.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "zKesRGIJV86O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Robustness check on the model : Ambiguity"
      ],
      "metadata": {
        "id": "iOnfuf9W645J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Define a set of ambiguous values for each column\n",
        "ambiguous_values = {\n",
        "    'column1': {\n",
        "        'topic': ['subject', 'summary', 'matter'],\n",
        "        'marriage': ['wedding', 'cermony', 'nuptial'],\n",
        "        'football': ['soccer', 'rudbey', 'grid game'],\n",
        "        'immigration': ['crossing', 'migration', 'departure'],\n",
        "        'advocacy': ['approval', 'defense', 'endorsement'],\n",
        "        # add more values as needed\n",
        "    },\n",
        "   \n",
        "}\n",
        "\n",
        "# Read the Excel dataset using Pandas\n",
        "df = pd.read_excel('d.xlsx')\n",
        "\n",
        "# Loop through each cell in the dataset and add ambiguity\n",
        "for index, row in df.iterrows():\n",
        "    for col in df.columns:\n",
        "        # Generate a random number between 0 and 1\n",
        "        rand_num = random.random()\n",
        "        if rand_num < 0.2:\n",
        "            # Replace the cell value with a random ambiguous value\n",
        "            ambiguous_values_for_col = ambiguous_values.get(col, {})\n",
        "            ambiguous_values_for_cell = ambiguous_values_for_col.get(row[col], [])\n",
        "            if ambiguous_values_for_cell:\n",
        "                df.at[index, col] = random.choice(ambiguous_values_for_cell)\n",
        "\n",
        "# Write the modified dataset back to Excel\n",
        "df.to_excel('ambiguity.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "641IgBGyYkYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hk-VfObU7OHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}